{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrBxKucQ6pHlcWio3lzCV6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stac-bot/Machine_learning_projects/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of words"
      ],
      "metadata": {
        "id": "U-_bpG7HmC29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Category:\n",
        "  BOOKS = 'BOOKS'\n",
        "  CLOTHING = 'CLOTHING'\n",
        "\n",
        "train_x =['i love the book', 'this is a great book', 'this fit is great', 'i love this shoes']\n",
        "train_y =[Category.BOOKS,Category.BOOKS, Category.CLOTHING, Category.CLOTHING]\n"
      ],
      "metadata": {
        "id": "B-4JUtiXk2h7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eDI-bB9YmHFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7zh5tFUbkBsG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "train_x_vectors = vectorizer.fit_transform(train_x)\n",
        "\n",
        "#print(vectorizer.get_feature_names_out())\n",
        "#print(vectors.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#good classic classifier model is svm\n",
        "from sklearn import svm\n",
        "\n",
        "clf_svm = svm.SVC(kernel='linear')\n",
        "clf_svm.fit(train_x_vectors,train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwLwiyEllop4",
        "outputId": "3e6cd63a-e0b0-4de9-a616-9fb51fe9609f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict new \n",
        "test_x = vectorizer.transform(['i like this shoes'])\n",
        "\n",
        "clf_svm.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z05avOK3m7Jv",
        "outputId": "f09bfde9-a461-4c26-e9a5-f35a09fcb1ab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CLOTHING'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word vectors "
      ],
      "metadata": {
        "id": "kKX6kocYpFZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Capture the semantic meaning of a word in a vector using  continous bag of word and spacy"
      ],
      "metadata": {
        "id": "KRgYKsiWpKOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "Vhr2lkdXn8WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "54P2B4Hfp4vw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_1bIFVxrz5G",
        "outputId": "c46bdbf6-d83c-4861-e74f-e4c921ce53fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i love the book', 'this is a great book', 'this fit is great', 'i love this shoes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [nlp(text) for text in train_x]\n",
        "train_x_word_vectors =[x.vector for x in docs]"
      ],
      "metadata": {
        "id": "-4dnmDjsqUNS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build the model for spcay\n",
        "from sklearn import svm\n",
        "\n",
        "clf_svm_wv = svm.SVC(kernel='linear')\n",
        "clf_svm_wv.fit(train_x_word_vectors,train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rux793UIrvxJ",
        "outputId": "8261d405-cdf3-4ade-e1fb-2cea8a4c30dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_x =[\"this ring hurts\"]\n",
        "test_docs=[nlp(text) for text in test_x]\n",
        "text_x_word_vectors=[x.vector for x in test_docs]\n",
        "\n",
        "clf_svm_wv.predict(text_x_word_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y94Wmtvrsfdu",
        "outputId": "dd634ded-5ee1-4d9d-a0e6-926f2651758f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CLOTHING'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. As the size grows the averaging may result to loose the real semantics of the context and the words\n",
        "2. \"check\" in context of bank and \"check\" in the context of checking something"
      ],
      "metadata": {
        "id": "qq7WLFSutptx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regexes\n",
        "\n",
        "1. pattern matching of string in python"
      ],
      "metadata": {
        "id": "cwTyF691uLS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "\n",
        "reg = re.compile(r\"\\bread\\b|story|book\")\n",
        "\n",
        "pharses = ['i love reading the books', 'this is a great book', 'this fit is great', 'i love this shoes']\n",
        "\n",
        "matches = []\n",
        "for p in pharses:\n",
        "  if re.search(reg,p):\n",
        "    matches.append(p)\n",
        "\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm6-Wa4hsotl",
        "outputId": "43c80969-1ebd-495d-e062-f06330086d2a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i love sdreading the books', 'this is a great book']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## stemming/Lemmatization"
      ],
      "metadata": {
        "id": "svLklGThwx1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "stemming use for connotations \n",
        "1. reading --> read\n",
        "\n",
        "and leamming  for spell checking\n",
        "1. stroi --> story"
      ],
      "metadata": {
        "id": "vE5QBCY4w5UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "Z9D03OErvrXe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9YPzb7UxK-Q",
        "outputId": "46a41fc5-8ab9-4f01-9f46-5cf0231a91fb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's start with the stemmer\n",
        "from nltk.tokenize import  word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "phrase = \"reading the books.\"\n",
        "words = word_tokenize(phrase)\n",
        "\n",
        "stemmed_words = []\n",
        "for word in words:\n",
        "  stemmed_words.append(stemmer.stem(word))\n",
        "\n",
        "stemmed_words\n",
        "\" \".join(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oXqUXRyUxYwW",
        "outputId": "a0716788-c6fc-498f-d9d9-f9b68e6fe5a9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'read the book .'"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizer"
      ],
      "metadata": {
        "id": "0ur6g7RGyiCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "phrase = \"reading the books\"\n",
        "words = word_tokenize(phrase)\n",
        "\n",
        "lemmatized_words = []\n",
        "for word in words:\n",
        "  lemmatized_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
        "\n",
        "\" \".join(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vbx98K0RyUDE",
        "outputId": "f23e1c5d-c8ce-4c46-c4cf-c1afe29efd0a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'read the book'"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopword removal"
      ],
      "metadata": {
        "id": "6vpiwdAkzk_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "#print(stop_words)\n",
        "\n",
        "phrase = \"here is an example sentence demonstrating the removal of stopwords\"\n",
        "\n",
        "words = word_tokenize(phrase)\n",
        "\n",
        "stripped_phrase = []\n",
        "for word in words:\n",
        "  if word not in stop_words:\n",
        "    stripped_phrase.append(word)\n",
        "\n",
        "' '.join(stripped_phrase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bvfYrIkPzANZ",
        "outputId": "408accb6-f164-4750-fa54-0c051ae1f0f8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'example sentence demonstrating removal stopwords'"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## various other technique(spell correction, sentiment and pos tagging)"
      ],
      "metadata": {
        "id": "3LAbeheq1Wsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-d52SJc2l-L",
        "outputId": "ae532789-43e7-4cb3-a18c-2fafc59873cf"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "phrase = \"i'm fucked and being dick at the same time\"\n",
        "\n",
        "tb_phrase = TextBlob(phrase)\n",
        "\n",
        "#correcting spellingss\n",
        "tb_phrase.correct()\n",
        "\n",
        "##part of speech tagging \n",
        "tb_phrase.tags\n",
        "\n",
        "##do the sentiment\n",
        "tb_phrase.sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJeQVgHwz4Sx",
        "outputId": "b7be5b33-26f3-4feb-cbf2-1c9e7c414e03"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=-0.3, subjectivity=0.4125)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i_wZ_cvH2FSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers Architecture"
      ],
      "metadata": {
        "id": "sIJ4E3MH37g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "20hky3g_4yC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_trf_bertbaseuncased_lg"
      ],
      "metadata": {
        "id": "jbdn5-Vu4FhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import torch\n",
        "\n",
        "#bert model\n",
        "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
        "doc = nlp(\"Here is some text to encode.\")"
      ],
      "metadata": {
        "id": "cpoobXBd4QT-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Category:\n",
        "  BOOKS = 'BOOKS'\n",
        "  BANK = \"BANK\"\n",
        "\n",
        "train_x =['good characters and plot progression', 'check out the book', 'good story. would recommend', 'novel recommendation','need to make deposit to the bank','blance inquiry savings', 'save money']\n",
        "train_y =[Category.BOOKS,Category.BOOKS,Category.BOOKS,Category.BOOKS,Category.BANK, Category.BANK, Category.BANK]\n"
      ],
      "metadata": {
        "id": "CALvxNL75Eg2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "docs = [nlp(text) for text in train_x]\n",
        "train_x_vectors =[doc.vector for doc in docs]\n",
        "clf_svm = svm.SVC(kernel = 'linear')\n",
        "clf_svm.fit(train_x_vectors, train_y)\n",
        "test_x =[\"check this story out\"]\n",
        "docs=[nlp(text) for text in test_x]\n",
        "text_x_vectors=[doc.vector for doc in docs]\n",
        "\n",
        "clf_svm.predict(text_x_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tYaeUff6sjZ",
        "outputId": "564064a3-b1b4-48b1-8171-acb3f6da9dce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['BOOKS'], dtype='<U5')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## homework\n",
        "1. be a social media analyitics\n",
        "checkout -- huggingface/tranformers to fine tune your model"
      ],
      "metadata": {
        "id": "EEL9p_508cfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vAY2d92s7_Bo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}